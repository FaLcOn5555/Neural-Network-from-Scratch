# Neural Network from Scratch using NumPy

This project demonstrates a neural network that solves the XOR problem using just Python and NumPy.

## ğŸš€ What It Does
- Builds a 2-layer neural network
- Solves XOR (non-linear problem)
- Uses sigmoid activation, forward & backpropagation, and gradient descent

## ğŸ§  Key Concepts
- Feedforward Neural Networks
- Sigmoid activation
- Mean Squared Error Loss
- Backpropagation
- Gradient Descent

---

## âš™ï¸ How to Set Up

### ğŸ“¥ Install Python Libraries

Make sure you have Python 3 and install required packages:

- bash
- python3
- pip install numpy matplotlib

## ğŸ”§Working Procedure

### ğŸ“¥ Input & Target
The model receives 4 binary input pairs and expected XOR outputs:

Input	Expected Output
<br>[0, 0]	0
<br>[0, 1]	1
<br>[1, 0]	1
<br>[1, 1]	0

### ğŸ§  What the Network Does
Initializes random weights and biases

- Applies forward propagation using sigmoid activation

- Calculates loss using Mean Squared Error (MSE)

- Backpropagates errors to update weights

- Repeats the process over 10,000 training epochs

## ğŸ“‰ Learning Process
During training:

The loss starts high (~0.27)

Gradually decreases

Reaches < 0.01 by the end â€” showing the network has learned

## ğŸ“Š Output Graph: Training Loss Curve
The graph below shows the loss steadily decreasing over epochs â€” confirming that the model is successfully learning the XOR mapping.


ğŸ”¹ Loss starts at ~0.27

ğŸ”¹ Sharp drop between 3,000â€“6,000 epochs

ğŸ”¹ Final loss is < 0.01

ğŸ“Œ This pattern shows excellent convergence for a non-linearly separable classification task.
<br><br>
![Loss Curve](loss_curve.png)


## âœ… Final Predictions

<br>Input: [0, 0] â†’ Output: ~0.05
<br>Input: [0, 1] â†’ Output: ~0.94
<br>Input: [1, 0] â†’ Output: ~0.94
<br>Input: [1, 1] â†’ Output: ~0.06
<br>ğŸ“Œ These predictions are very close to the true XOR outputs â€” showing that the neural network learned the logic correctly.


